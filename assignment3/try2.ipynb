{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def a3(wd_coefficient, n_hid, n_iters, learning_rate,\n",
    "        momentum_multiplier, do_early_stopping, mini_batch_size):\n",
    "\n",
    "    model = initial_model(n_hid)\n",
    "\n",
    "    # Load data\n",
    "    from_data_file = sio.loadmat('data.mat')['data']\n",
    "    training_data = {}\n",
    "    training_data['inputs'] = from_data_file[0][0]['training'][0][0]['inputs']\n",
    "    training_data['targets'] = from_data_file[0][0]['training'][0][0]['targets']\n",
    "    validation_data = {}\n",
    "    validation_data['inputs'] = from_data_file[0][0]['validation'][0][0]['inputs']\n",
    "    validation_data['targets'] = from_data_file[0][0]['validation'][0][0]['targets']\n",
    "    test_data = {}\n",
    "    test_data['inputs'] = from_data_file[0][0]['test'][0][0]['inputs']\n",
    "    test_data['targets'] = from_data_file[0][0]['test'][0][0]['targets']\n",
    "\n",
    "    n_training_cases = training_data['inputs'].shape[1]\n",
    "#     if n_iters != 0:\n",
    "#         test_gradient(model, training_data, wd_coefficient)\n",
    "\n",
    "    # Optimization\n",
    "    theta = model_to_theta(model)\n",
    "    momentum_speed = theta * 0\n",
    "    training_data_losses = []\n",
    "    validation_data_losses = []\n",
    "    best_so_far = {}\n",
    "    if do_early_stopping:\n",
    "        best_so_far['theta'] = -1 # this will be overwritten soon\n",
    "        best_so_far['validation_loss'] = np.inf\n",
    "        best_so_far['after_n_iters'] = -1\n",
    "\n",
    "    for optimization_iteration_i in range(1,n_iters+1):\n",
    "        model = theta_to_model(theta)\n",
    "        training_batch = {}\n",
    "        training_batch_start = ((optimization_iteration_i-1) * mini_batch_size)\\\n",
    "                               % n_training_cases + 1\n",
    "        training_batch['inputs'] = training_data['inputs'][:,\n",
    "            training_batch_start-1 : training_batch_start-1 + mini_batch_size ]\n",
    "        training_batch['targets'] = training_data['targets'][:,\n",
    "            training_batch_start-1 : training_batch_start-1 + mini_batch_size ]\n",
    "        gradient = model_to_theta(d_loss_by_d_model(model,\n",
    "                    training_batch, wd_coefficient))\n",
    "        momentum_speed = momentum_speed * momentum_multiplier - gradient\n",
    "        theta = theta + momentum_speed * learning_rate\n",
    "\n",
    "        model = theta_to_model(theta)\n",
    "        training_data_losses.append(loss(model, training_data, wd_coefficient))\n",
    "        validation_data_losses.append(loss(model, validation_data, wd_coefficient))\n",
    "\n",
    "        if do_early_stopping and \\\n",
    "            validation_data_losses[-1] < best_so_far['validation_loss']:\n",
    "          best_so_far['theta'] = theta # this will be overwritten soon\n",
    "          best_so_far['validation_loss'] = validation_data_losses[-1]\n",
    "          best_so_far['after_n_iters'] = optimization_iteration_i\n",
    "\n",
    "#         if optimization_iteration_i % round(n_iters/10) == 0:\n",
    "#             print('After ', optimization_iteration_i, 'optimization iterations,'\n",
    "#                 ' training data loss is', training_data_losses[-1],\n",
    "#                 ', and validation data loss is ', validation_data_losses[-1])\n",
    "\n",
    "#     if n_iters != 0:\n",
    "#         # check again, this time with more typical parameters\n",
    "#         test_gradient(model, training_data, wd_coefficient)\n",
    "\n",
    "    if do_early_stopping:\n",
    "        print('Early stopping: validation loss was lowest after ',\n",
    "            best_so_far['after_n_iters'],  'iterations. We chose the '\n",
    "            'model that we had then.')\n",
    "        theta = best_so_far['theta']\n",
    "\n",
    "\n",
    "    # The optimization is finished. Now do some reporting.\n",
    "    model = theta_to_model(theta)\n",
    "\n",
    "    if n_iters != 0:\n",
    "        plt.figure(1)\n",
    "        plt.clf()\n",
    "        plt.plot(range(n_iters),training_data_losses, 'b')\n",
    "        plt.plot(range(n_iters),validation_data_losses, 'r')\n",
    "        plt.legend(['training', 'validation'])\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('iteration number')\n",
    "        plt.show()\n",
    "\n",
    "    datas2 = [training_data, validation_data, test_data]\n",
    "    data_names = ['training', 'validation', 'test']\n",
    "    for data_i in range (0,3):\n",
    "        data = datas2[data_i]\n",
    "        data_name = data_names[data_i]\n",
    "        print('The loss on the ', data_name, \"data is \",\n",
    "              loss(model, data, wd_coefficient))\n",
    "        if wd_coefficient != 0:\n",
    "            print('The classification loss (i.e. without weight decay) on the ',\n",
    "                data_name, \"data is \", loss(model, data, 0))\n",
    "        print('The classification error rate on the', data_name, 'data is ',\n",
    "            classification_performance(model, data))\n",
    "\n",
    "def test_gradient(model, data, wd_coefficient):\n",
    "    base_theta = model_to_theta(model)\n",
    "    h = 0.01\n",
    "    correctness_threshold = 0.00001\n",
    "    analytic_gradient = model_to_theta(\n",
    "                d_loss_by_d_model(model, data, wd_coefficient))\n",
    "    # Test the gradient not for every element of theta, because that's\n",
    "    # a lot of work. Test for only a few elements.\n",
    "    for i in range(1,101):\n",
    "\n",
    "        # 1299721 is prime and thus ensures a somewhat random-like\n",
    "        # selection of indices\n",
    "        test_index = (i * 1299721) % base_theta.shape[0]\n",
    "\n",
    "        analytic_here = analytic_gradient[test_index]\n",
    "        theta_step = base_theta * 0\n",
    "        theta_step[test_index] = h\n",
    "        contribution_distances = [-4, -3, -2, -1, 1, 2, 3, 4]\n",
    "        contribution_weights = [1/280, -4/105, 1/5, -4/5, 4/5, -1/5, 4/105, -1/280]\n",
    "        temp = 0\n",
    "        for contribution_index in range(8):\n",
    "            temp = temp + loss(theta_to_model(\n",
    "                    base_theta +\n",
    "                    theta_step * contribution_distances[contribution_index]),\n",
    "                data,\n",
    "                wd_coefficient) * contribution_weights[contribution_index]\n",
    "\n",
    "        fd_here = temp / h\n",
    "        diff = np.abs(analytic_here - fd_here)\n",
    "        if diff < correctness_threshold:\n",
    "            continue\n",
    "        if diff / (np.abs(analytic_here) + np.abs(fd_here)) < \\\n",
    "                        correctness_threshold:\n",
    "            continue\n",
    "\n",
    "        print('Error! Theta element #', test_index+1 , ', with value ',\n",
    "            base_theta[test_index], 'has finite difference gradient', fd_here,\n",
    "            'but analytic gradient is', analytic_here,'That looks like an error.')\n",
    "        quit()\n",
    "\n",
    "    print('Gradient test passed. That means that the gradient that your '\n",
    "          'code computed is within 0.001% of the gradient that the finite'\n",
    "          ' difference approximation computed, so the gradient calculation'\n",
    "          ' procedure is probably correct (not certainly, but probably).')\n",
    "\n",
    "def logistic(input):\n",
    "    return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def log_sum_exp_over_rows(a):\n",
    "    # This computes log(sum(exp(a), axis=0)) in a numerically stable way\n",
    "    maxs_small = np.reshape(np.amax(a, axis=0),(1,-1), order=\"F\")\n",
    "    maxs_big = np.tile(maxs_small, (a.shape[0], 1))\n",
    "    ret = np.log(np.sum(np.exp(a - maxs_big), axis=0)) + maxs_small\n",
    "    return ret\n",
    "\n",
    "def loss(model, data, wd_coefficient):\n",
    "    # model['input_to_hid'] is a matrix of size <number of hidden units> by\n",
    "    # <number of inputs i.e. 256>. It contains the weights from the input\n",
    "    # units to the hidden units.\n",
    "    # model['hid_to_class'] is a matrix of size <number of classes i.e. 10>\n",
    "    # by <number of hidden units>. It contains the weights from the hidden\n",
    "    # units to the softmax units.\n",
    "    # data['inputs'] is a matrix of size <number of inputs i.e. 256> by\n",
    "    # <number of data cases>. Each column describes a different data case.\n",
    "    # data['targets'] is a matrix of size <number of classes i.e. 10> by\n",
    "    # <number of data cases>. Each column describes a different data case.\n",
    "    # It contains a one-of-N encoding of the class, i.e. one element in\n",
    "    # every column is 1 and the others are 0.\n",
    "\n",
    "    # Before we can calculate the loss, we need to calculate a variety\n",
    "    # of intermediate values, like the state of the hidden units.\n",
    "\n",
    "    # input to the hidden units, i.e. before the logistic.\n",
    "    # size: <number of hidden units> by <number of data cases>\n",
    "    hid_input = np.dot(model['input_to_hid'], data['inputs'])\n",
    "\n",
    "    # output of the hidden units, i.e. after the logistic.\n",
    "    # size: <number of hidden units> by <number of data cases>\n",
    "    hid_output = logistic(hid_input)\n",
    "\n",
    "    # input to the components of the softmax.\n",
    "    # size: <number of classes, i.e. 10> by <number of data cases>\n",
    "    class_input = np.dot(model['hid_to_class'], hid_output)\n",
    "\n",
    "    # The following three lines of code implement the softmax.\n",
    "    # However, it's written differently from what the lectures say.\n",
    "    # In the lectures, a softmax is described using an exponential\n",
    "    # divided by a sum of exponentials.\n",
    "    # What we do here is exactly equivalent (you can check the math or\n",
    "    # just check it in practice), but this is more numerically stable.\n",
    "    # \"Numerically stable\" means that this way, there will never be really\n",
    "    # big numbers involved.\n",
    "    # The exponential in the lectures can lead to really big numbers, which\n",
    "    # are fine in mathematical equations, but can lead to all sorts of\n",
    "    # problems in Octave.\n",
    "    # Octave isn't well prepared to deal with really large numbers, like\n",
    "    # the number 10 to the power 1000. Computations with such numbers get\n",
    "    # unstable, so we avoid them.\n",
    "\n",
    "    # log(sum(exp of class_input)) is what we subtract to get properly\n",
    "    # normalized log class probabilities. size: <1> by <number of data cases>\n",
    "    class_normalizer = log_sum_exp_over_rows(class_input)\n",
    "\n",
    "    # log of probability of each class. size: <number of classes, i.e. 10>\n",
    "    # by <number of data cases>\n",
    "    log_class_prob = class_input - \\\n",
    "                     np.tile(class_normalizer, (class_input.shape[0], 1))\n",
    "\n",
    "    # probability of each class. Each column (i.e. each case) sums to 1.\n",
    "    # size: <number of classes, i.e. 10> by <number of data cases>\n",
    "    class_prob = np.exp(log_class_prob)\n",
    "\n",
    "    # select the right log class probability using that sum; then take the\n",
    "    # mean over all data cases.\n",
    "    classification_loss = -np.mean(np.sum(log_class_prob * data['targets'],\n",
    "                                          axis=0))\n",
    "\n",
    "    # weight decay loss. very straightforward: E = 1/2 * wd_coeffecient * theta^2\n",
    "    wd_loss = np.sum(model_to_theta(model)*model_to_theta(model))/2*wd_coefficient\n",
    "\n",
    "    ret = classification_loss + wd_loss\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "def model_to_theta(model):\n",
    "    # This function takes a model (or gradient in model form),\n",
    "    # and turns it into one long vector. See also theta_to_model.\n",
    "    input_to_hid_transpose = np.transpose(model['input_to_hid'])\n",
    "    hid_to_class_transpose = np.transpose(model['hid_to_class'])\n",
    "\n",
    "    ret = np.concatenate((input_to_hid_transpose.flatten('F'),\n",
    "                          hid_to_class_transpose.flatten('F')), axis=0)\n",
    "    return ret\n",
    "\n",
    "def theta_to_model(theta):\n",
    "    # This function takes a model (or gradient) in the form of one long\n",
    "    # vector (maybe produced by model_to_theta), and restores it to the\n",
    "    # structure format, i.e. with fields .input_to_hid and .hid_to_class,\n",
    "    # both matrices.\n",
    "\n",
    "    ret = {}\n",
    "\n",
    "    n_hid = int(theta.shape[0] / (256+10))\n",
    "    ret['input_to_hid'] = np.transpose(np.reshape(\n",
    "                    theta[0:256*n_hid],(256, n_hid), order=\"F\"))\n",
    "    ret['hid_to_class'] = np.transpose(\n",
    "            np.reshape(theta[256*n_hid:theta.shape[0]], (n_hid, 10), order=\"F\"))\n",
    "\n",
    "    return ret\n",
    "\n",
    "def initial_model(n_hid):\n",
    "    n_params = (256+10) * n_hid\n",
    "    as_row_vector = np.cos(np.asarray(range(n_params)))\n",
    "\n",
    "    # We don't use random initialization, for this assignment.\n",
    "    # This way, everybody will get the same results.\n",
    "    ret = theta_to_model(as_row_vector * 0.1)\n",
    "    return ret\n",
    "\n",
    "def classification_performance(model, data):\n",
    "    # This returns the fraction of data cases that is incorrectly\n",
    "    # classified by the model.\n",
    "\n",
    "    # input to the hidden units, i.e. before the logistic.\n",
    "    # size: <number of hidden units> by <number of data cases>\n",
    "    hid_input = np.dot(model['input_to_hid'], data['inputs'])\n",
    "\n",
    "    # output of the hidden units, i.e. after the logistic.\n",
    "    # size: <number of hidden units> by <number of data cases>\n",
    "    hid_output = logistic(hid_input)\n",
    "\n",
    "    # input to the components of the softmax. size: <number of classes,\n",
    "    # i.e. 10> by <number of data cases>\n",
    "    class_input = np.dot(model['hid_to_class'], hid_output)\n",
    "\n",
    "    # choices is integer: the chosen class, plus 1.\n",
    "    choices = np.argmax(class_input, axis=0)\n",
    "\n",
    "    # targets is integer: the target class, plus 1.\n",
    "    targets = np.argmax(data['targets'], axis=0)\n",
    "\n",
    "    sumtmp = 0.\n",
    "    for i in range(choices.shape[0]):\n",
    "        if choices[i] != targets[i]:\n",
    "            sumtmp = sumtmp + 1.\n",
    "        else:\n",
    "            sumtmp = sumtmp + 0.\n",
    "\n",
    "    ret = sumtmp / choices.shape[0]\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def d_loss_by_d_model(model, data, wd_coefficient):\n",
    "    # model['input_to_hid'] is a matrix of size <number of hidden units>\n",
    "    # by <number of inputs i.e. 256>\n",
    "    # model['hid_to_class'] is a matrix of size <number of classes i.e. 10>\n",
    "    # by <number of hidden units>\n",
    "    # data['inputs'] is a matrix of size <number of inputs i.e. 256> by\n",
    "    # <number of data cases>. Each column describes a different data case.\n",
    "    # data['targets'] is a matrix of size <number of classes i.e. 10> by\n",
    "    # <number of data cases>. Each column describes a different data case.\n",
    "    # It contains a one-of-N encoding of the class, i.e. one element in\n",
    "    # every column is 1 and the others are 0.\n",
    "    #\n",
    "    # The returned object is supposed to be exactly like parameter <model>,\n",
    "    # i.e. it has fields ret.input_to_hid and ret.hid_to_class. However,\n",
    "    # the contents of those matrices are gradients (d loss by d model\n",
    "    # parameter), instead of model parameters.\n",
    "    #\n",
    "    # This is the only function that you're expected to change. Right now,\n",
    "    # it just returns a lot of zeros, which is obviously not the correct\n",
    "    # output. Your job is to replace that by a correct computation.\n",
    "\n",
    "    ret = {}\n",
    "    dcdwih =np.zeros(model['input_to_hid'].shape)\n",
    "#     print(dcdwih.shape)\n",
    "    dcdwhs =np.zeros(model['hid_to_class'].shape)\n",
    "    number_of_cases = len(data['targets'][0])\n",
    "   \n",
    "    W1 = model['input_to_hid']\n",
    "    W2 = model['hid_to_class']\n",
    "    for j in range(number_of_cases):\n",
    "        X = (data['inputs'].T)[j].reshape(data['inputs'].T.shape[1],1)\n",
    "        Y = data['targets'].T[j].reshape(data['targets'].T.shape[1],1)\n",
    "        Z1 = W1@(X)\n",
    "        A1 = 1/(1+np.exp(-Z1))\n",
    "        Z2 = W2@A1\n",
    "        A2 = np.exp(Z2)/(np.sum(np.exp(Z2)))\n",
    "#         print((A2-Y).shape)\n",
    "#         print(W2.shape)\n",
    "#         print((A1.T@(1-A1)).shape)\n",
    "#         print(X.shape)\n",
    "        \n",
    "        dcdwhs = dcdwhs + (A2-Y)@A1.T\n",
    "        dcdwih = dcdwih + ((A2-Y).T@W2).T@(A1.T@(1-A1)).T@X.T\n",
    "        \n",
    "#     print(model_hc.shape)\n",
    "\n",
    "    ret['input_to_hid'] = dcdwih/number_of_cases + wd_coefficient*model['input_to_hid']\n",
    "    ret['hid_to_class'] = dcdwhs/number_of_cases + wd_coefficient*model['hid_to_class']\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error! Theta element # 1642 , with value  0.046369533543 has finite difference gradient -0.00269888772205 but analytic gradient is -0.0269754389338 That looks like an error.\n",
      "Error! Theta element # 623 , with value  0.0999375415997 has finite difference gradient -0.000629245939724 but analytic gradient is -0.00629429336853 That looks like an error.\n",
      "Error! Theta element # 2264 , with value  0.0494715045845 has finite difference gradient 0.00129069669896 but analytic gradient is 0.0128815619546 That looks like an error.\n",
      "Error! Theta element # 1245 , with value  0.0997502444198 has finite difference gradient -0.0009895107284 but analytic gradient is -0.0099055707942 That looks like an error.\n",
      "Error! Theta element # 226 , with value  0.036731936773 has finite difference gradient -0.000833860505686 but analytic gradient is -0.00832806879328 That looks like an error.\n",
      "Error! Theta element # 1867 , with value  0.0994383424259 has finite difference gradient -0.000761337552256 but analytic gradient is -0.0076069735872 That looks like an error.\n",
      "Error! Theta element # 848 , with value  0.0334222205001 has finite difference gradient -0.000592997167076 but analytic gradient is -0.00592500764405 That looks like an error.\n",
      "Error! Theta element # 2489 , with value  0.099002225236 has finite difference gradient -9.29847414689e-05 but analytic gradient is -0.00091473250381 That looks like an error.\n",
      "Error! Theta element # 1470 , with value  0.0300707542586 has finite difference gradient -0.00120165118862 but analytic gradient is -0.0120276848272 That looks like an error.\n",
      "Error! Theta element # 451 , with value  -0.0730152964181 has finite difference gradient 0.0013665077592 but analytic gradient is 0.0136766880287 That looks like an error.\n",
      "Error! Theta element # 2092 , with value  0.026681724593 has finite difference gradient 0.00116750798534 but analytic gradient is 0.0116529569936 That looks like an error.\n",
      "Error! Theta element # 1073 , with value  -0.0753842837952 has finite difference gradient 3.28658874681e-06 but analytic gradient is 3.0482208734e-05 That looks like an error.\n",
      "Error! Theta element # 54 , with value  -0.0918282786212 has finite difference gradient 0.000903713722067 but analytic gradient is 0.00907350249501 That looks like an error.\n",
      "Error! Theta element # 1695 , with value  -0.0776591035369 has finite difference gradient -0.00053249290515 but analytic gradient is -0.00532491823299 That looks like an error.\n",
      "Error! Theta element # 676 , with value  -0.0903718018029 has finite difference gradient 0.00111382980228 but analytic gradient is 0.0111278858484 That looks like an error.\n",
      "Error! Theta element # 2317 , with value  -0.079836914011 has finite difference gradient -0.000235090553643 but analytic gradient is -0.00234782847124 That looks like an error.\n",
      "Error! Theta element # 1298 , with value  -0.088802435421 has finite difference gradient 0.000160378966129 but analytic gradient is 0.00160048773627 That looks like an error.\n",
      "Error! Theta element # 279 , with value  0.00309449018283 has finite difference gradient 0.00109926525679 but analytic gradient is 0.0109590255981 That looks like an error.\n",
      "Error! Theta element # 1920 , with value  -0.0871221398779 has finite difference gradient 0.000448191172999 but analytic gradient is 0.00448312853212 That looks like an error.\n",
      "Error! Theta element # 901 , with value  0.00662467022032 has finite difference gradient 0.0011771238086 but analytic gradient is 0.011792275802 That looks like an error.\n",
      "Error! Theta element # 2542 , with value  -0.0853330141451 has finite difference gradient -4.82657332832e-05 but analytic gradient is -0.000473207238881 That looks like an error.\n",
      "Error! Theta element # 1523 , with value  0.0101465749317 has finite difference gradient -0.00127586469339 but analytic gradient is -0.0127610993332 That looks like an error.\n",
      "Error! Theta element # 504 , with value  0.094101610976 has finite difference gradient 0.0013778464833 but analytic gradient is 0.0137607518262 That looks like an error.\n",
      "Error! Theta element # 2145 , with value  0.0136558048663 has finite difference gradient -0.000632799891867 but analytic gradient is -0.00631945486484 That looks like an error.\n",
      "Error! Theta element # 1126 , with value  0.0952385411563 has finite difference gradient -0.000764028959152 but analytic gradient is -0.00764396450813 That looks like an error.\n",
      "Error! Theta element # 107 , with value  0.0686486550907 has finite difference gradient -0.00163153697926 but analytic gradient is -0.016288853692 That looks like an error.\n",
      "Error! Theta element # 1748 , with value  0.0962565023981 has finite difference gradient -0.000734917877112 but analytic gradient is -0.00731277400529 That looks like an error.\n",
      "Error! Theta element # 729 , with value  0.0660361983094 has finite difference gradient 0.00107433963565 but analytic gradient is 0.0107095779494 That looks like an error.\n",
      "Error! Theta element # 2370 , with value  0.0971542230966 has finite difference gradient -0.000567322310725 but analytic gradient is -0.00568353363254 That looks like an error.\n",
      "Error! Theta element # 1351 , with value  0.0633412512219 has finite difference gradient -0.000948482449988 but analytic gradient is -0.00948957583784 That looks like an error.\n",
      "Error! Theta element # 332 , with value  -0.042415170907 has finite difference gradient -0.00104656253786 but analytic gradient is -0.0104621497824 That looks like an error.\n",
      "Error! Theta element # 1973 , with value  0.0605671802699 has finite difference gradient -0.0016609876825 but analytic gradient is -0.0166181247559 That looks like an error.\n",
      "Error! Theta element # 954 , with value  -0.0455888617554 has finite difference gradient -0.000219404107107 but analytic gradient is -0.00218053801137 That looks like an error.\n",
      "Error! Theta element # 1576 , with value  -0.0487056044562 has finite difference gradient 0.00165434017129 but analytic gradient is 0.0165837544696 That looks like an error.\n",
      "Error! Theta element # 557 , with value  -0.0998084826102 has finite difference gradient 0.000839680792412 but analytic gradient is 0.00837818083695 That looks like an error.\n",
      "Error! Theta element # 2198 , with value  -0.0517615056742 has finite difference gradient 0.000741543073517 but analytic gradient is 0.00740196616023 That looks like an error.\n",
      "Error! Theta element # 1179 , with value  -0.0995275420605 has finite difference gradient 0.000949119722001 but analytic gradient is 0.00947372244099 That looks like an error.\n",
      "Error! Theta element # 160 , with value  -0.0342494779116 has finite difference gradient -0.000145840495336 but analytic gradient is -0.00146061674944 That looks like an error.\n",
      "Error! Theta element # 1801 , with value  -0.0991222748894 has finite difference gradient 0.000660142372953 but analytic gradient is 0.00658616206837 That looks like an error.\n",
      "Error! Theta element # 782 , with value  -0.030908006546 has finite difference gradient -0.000147049770012 but analytic gradient is -0.00145546324113 That looks like an error.\n",
      "Error! Theta element # 2423 , with value  -0.0985931873439 has finite difference gradient 0.00259627319152 but analytic gradient is 0.0259744865978 That looks like an error.\n",
      "Error! Theta element # 1404 , with value  -0.0275279258874 has finite difference gradient 0.000535735028902 but analytic gradient is 0.00535730002525 That looks like an error.\n",
      "Error! Theta element # 385 , with value  0.0748037524535 has finite difference gradient -0.000411358856481 but analytic gradient is -0.00411807072022 That looks like an error.\n",
      "Error! Theta element # 2026 , with value  -0.0241134582245 has finite difference gradient 0.00177825581565 but analytic gradient is 0.0177629778563 That looks like an error.\n",
      "Error! Theta element # 1007 , with value  0.0771022637656 has finite difference gradient -0.000267833040447 but analytic gradient is -0.00266435786639 That looks like an error.\n",
      "Error! Theta element # 1629 , with value  0.0793044613967 has finite difference gradient 3.78687203815e-05 but analytic gradient is 0.000390540322196 That looks like an error.\n",
      "Error! Theta element # 610 , with value  0.0892032120217 has finite difference gradient -0.00140135532166 but analytic gradient is -0.0140042739207 That looks like an error.\n",
      "Error! Theta element # 2251 , with value  0.0814075944318 has finite difference gradient 0.000808471029154 but analytic gradient is 0.00807620435389 That looks like an error.\n",
      "Error! Theta element # 1232 , with value  0.0875503160999 has finite difference gradient -0.00013777727799 but analytic gradient is -0.00137946537093 That looks like an error.\n",
      "Error! Theta element # 213 , with value  -0.00574724308477 has finite difference gradient 3.89529835276e-05 but analytic gradient is 0.000424981710928 That looks like an error.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error! Theta element # 1854 , with value  0.0857880551242 has finite difference gradient 0.000240479684702 but analytic gradient is 0.00240072409746 That looks like an error.\n",
      "Error! Theta element # 835 , with value  -0.00927161757483 has finite difference gradient -0.000399695466356 but analytic gradient is -0.00399351686877 That looks like an error.\n",
      "Error! Theta element # 2476 , with value  0.0839186304548 has finite difference gradient -0.000443521268771 but analytic gradient is -0.00442408245523 That looks like an error.\n",
      "Error! Theta element # 1457 , with value  -0.0127844102569 has finite difference gradient -0.000609187460835 but analytic gradient is -0.0060958876298 That looks like an error.\n",
      "Error! Theta element # 438 , with value  -0.0949668255372 has finite difference gradient -0.00216526418477 but analytic gradient is -0.0216624901097 That looks like an error.\n",
      "Error! Theta element # 2079 , with value  -0.0162812330626 has finite difference gradient 0.00053577734044 but analytic gradient is 0.00536010432398 That looks like an error.\n",
      "Error! Theta element # 1060 , with value  -0.0960145014326 has finite difference gradient -0.000156011650263 but analytic gradient is -0.00158339143905 That looks like an error.\n",
      "Error! Theta element # 41 , with value  -0.0666938061652 has finite difference gradient 0.00240651998942 but analytic gradient is 0.0241101263522 That looks like an error.\n",
      "Error! Theta element # 1682 , with value  -0.0969422390846 has finite difference gradient 3.36966307435e-05 but analytic gradient is 0.000341945833269 That looks like an error.\n",
      "Error! Theta element # 663 , with value  -0.0640190655386 has finite difference gradient 0.000943399089293 but analytic gradient is 0.00941221937928 That looks like an error.\n",
      "Error! Theta element # 2304 , with value  -0.0977488795931 has finite difference gradient -0.000124430542188 but analytic gradient is -0.00124725296855 That looks like an error.\n",
      "Error! Theta element # 1285 , with value  -0.0612643543435 has finite difference gradient -4.75590286535e-05 but analytic gradient is -0.000485237635187 That looks like an error.\n",
      "Error! Theta element # 266 , with value  0.0448046666974 has finite difference gradient 0.000688606879478 but analytic gradient is 0.00686333750996 That looks like an error.\n",
      "Error! Theta element # 1907 , with value  -0.058433113677 has finite difference gradient -0.000622702632838 but analytic gradient is -0.00625402676088 That looks like an error.\n",
      "Error! Theta element # 888 , with value  0.0479359401846 has finite difference gradient 0.00291281297297 but analytic gradient is 0.0291367734151 That looks like an error.\n",
      "Error! Theta element # 2529 , with value  -0.0555288802343 has finite difference gradient 0.000562180247647 but analytic gradient is 0.00562039271124 That looks like an error.\n",
      "Error! Theta element # 1510 , with value  0.0510073336289 has finite difference gradient -0.00350232566828 but analytic gradient is -0.0350367365514 That looks like an error.\n",
      "Error! Theta element # 491 , with value  0.0996090498506 has finite difference gradient 0.00167758353204 but analytic gradient is 0.0167518592296 That looks like an error.\n",
      "Error! Theta element # 2132 , with value  0.0540150103439 has finite difference gradient -0.000103896154269 but analytic gradient is -0.00102822998534 That looks like an error.\n",
      "Error! Theta element # 1113 , with value  0.0992346640189 has finite difference gradient 7.11800713496e-05 but analytic gradient is 0.000700087359652 That looks like an error.\n",
      "Error! Theta element # 94 , with value  0.031742870152 has finite difference gradient 0.00142036441421 but analytic gradient is 0.0142196898826 That looks like an error.\n",
      "Error! Theta element # 1735 , with value  0.0987363174198 has finite difference gradient -0.000188588403787 but analytic gradient is -0.00184809887715 That looks like an error.\n",
      "Error! Theta element # 716 , with value  0.0283719997252 has finite difference gradient 0.00091880312871 but analytic gradient is 0.00917703372873 That looks like an error.\n",
      "Error! Theta element # 2357 , with value  0.0981146325719 has finite difference gradient -0.000880817756631 but analytic gradient is -0.00878426245897 That looks like an error.\n",
      "Error! Theta element # 1338 , with value  0.0249656879041 has finite difference gradient -0.000696366399194 but analytic gradient is -0.00697146610002 That looks like an error.\n",
      "Error! Theta element # 319 , with value  -0.0765394652557 has finite difference gradient 0.000842863924143 but analytic gradient is 0.00842570993293 That looks like an error.\n",
      "Error! Theta element # 1960 , with value  0.0215281897444 has finite difference gradient -0.0020729237853 but analytic gradient is -0.0207417447345 That looks like an error.\n",
      "Error! Theta element # 941 , with value  -0.0787658798499 has finite difference gradient -0.00093916731255 but analytic gradient is -0.00937075191235 That looks like an error.\n",
      "Error! Theta element # 1563 , with value  -0.0808939026271 has finite difference gradient 0.00208166016293 but analytic gradient is 0.0208443757526 That looks like an error.\n",
      "Error! Theta element # 544 , with value  -0.0879717261202 has finite difference gradient -3.02179382067e-05 but analytic gradient is -0.000300670018911 That looks like an error.\n",
      "Error! Theta element # 2185 , with value  -0.0829208753292 has finite difference gradient 0.000195529503558 but analytic gradient is 0.00190688324069 That looks like an error.\n",
      "Error! Theta element # 1166 , with value  -0.0862364660956 has finite difference gradient -0.00136922056517 but analytic gradient is -0.0136980186549 That looks like an error.\n",
      "Error! Theta element # 147 , with value  0.00839594367418 has finite difference gradient -0.000271606662683 but analytic gradient is -0.00270285782031 That looks like an error.\n",
      "Error! Theta element # 1788 , with value  -0.0843934822365 has finite difference gradient -7.54211854904e-05 but analytic gradient is -0.000745149087885 That looks like an error.\n",
      "Error! Theta element # 769 , with value  0.0119120276224 has finite difference gradient -0.00015006236178 but analytic gradient is -0.00149982894858 That looks like an error.\n",
      "Error! Theta element # 2410 , with value  -0.0824450767395 has finite difference gradient 0.0027703257311 but analytic gradient is 0.0277133149995 That looks like an error.\n",
      "Error! Theta element # 1391 , with value  0.0154132314469 has finite difference gradient 0.000340272614809 but analytic gradient is 0.00340284794274 That looks like an error.\n",
      "Error! Theta element # 372 , with value  0.0957650801229 has finite difference gradient -2.44737823356e-05 but analytic gradient is -0.000269539024725 That looks like an error.\n",
      "Error! Theta element # 2013 , with value  0.0188951815557 has finite difference gradient 0.000897835399255 but analytic gradient is 0.00896592478093 That looks like an error.\n",
      "Error! Theta element # 994 , with value  0.0967227630295 has finite difference gradient 0.00113337718321 but analytic gradient is 0.0113445520661 That looks like an error.\n",
      "Error! Theta element # 1616 , with value  0.0975596229551 has finite difference gradient 0.000556770608575 but analytic gradient is 0.00557276945764 That looks like an error.\n",
      "Error! Theta element # 597 , with value  0.0619567936886 has finite difference gradient 0.000599559830395 but analytic gradient is 0.0059962326223 That looks like an error.\n",
      "Error! Theta element # 2238 , with value  0.098274614521 has finite difference gradient 0.00124819084081 but analytic gradient is 0.0124799728734 That looks like an error.\n",
      "Error! Theta element # 1219 , with value  0.0591442606998 has finite difference gradient -0.00114175136614 but analytic gradient is -0.0114140048253 That looks like an error.\n",
      "Error! Theta element # 200 , with value  -0.047162571252 has finite difference gradient -3.54195056132e-06 but analytic gradient is -1.59887865205e-05 That looks like an error.\n",
      "Error! Theta element # 1841 , with value  0.0562578465928 has finite difference gradient 1.27330822849e-05 but analytic gradient is 0.000125156768949 That looks like an error.\n",
      "Gradient test passed. That means that the gradient that your code computed is within 0.001% of the gradient that the finite difference approximation computed, so the gradient calculation procedure is probably correct (not certainly, but probably).\n",
      "After  7 optimization iterations, training data loss is 2.30375939536 , and validation data loss is  2.30386659674\n",
      "After  14 optimization iterations, training data loss is 2.30215550774 , and validation data loss is  2.30234317493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After  21 optimization iterations, training data loss is 2.30094825708 , and validation data loss is  2.30122138346\n",
      "After  28 optimization iterations, training data loss is 2.29995563575 , and validation data loss is  2.30027906771\n",
      "After  35 optimization iterations, training data loss is 2.29908065068 , and validation data loss is  2.29941342445\n",
      "After  42 optimization iterations, training data loss is 2.29791892122 , and validation data loss is  2.29832249339\n",
      "After  49 optimization iterations, training data loss is 2.29679760351 , and validation data loss is  2.29729039242\n",
      "After  56 optimization iterations, training data loss is 2.29596382508 , and validation data loss is  2.29648480303\n",
      "After  63 optimization iterations, training data loss is 2.29516078074 , and validation data loss is  2.29576295038\n",
      "After  70 optimization iterations, training data loss is 2.29439873649 , and validation data loss is  2.29500096722\n",
      "Error! Theta element # 1642 , with value  0.0549317379555 has finite difference gradient -0.00263120050762 but analytic gradient is -0.0263482305028 That looks like an error.\n",
      "Error! Theta element # 623 , with value  0.101000165148 has finite difference gradient -0.000691358936927 but analytic gradient is -0.00686810123866 That looks like an error.\n",
      "Error! Theta element # 2264 , with value  0.0455819060751 has finite difference gradient 0.00111941281367 but analytic gradient is 0.0113041184593 That looks like an error.\n",
      "Error! Theta element # 1245 , with value  0.105373280098 has finite difference gradient -0.000922565047471 but analytic gradient is -0.00932540625251 That looks like an error.\n",
      "Error! Theta element # 226 , with value  0.0395527894134 has finite difference gradient -0.000819716762613 but analytic gradient is -0.00824545276201 That looks like an error.\n",
      "Error! Theta element # 1867 , with value  0.103316916748 has finite difference gradient -0.000805588233444 but analytic gradient is -0.0079532780795 That looks like an error.\n",
      "Error! Theta element # 848 , with value  0.0351514766724 has finite difference gradient -0.000598017301954 but analytic gradient is -0.00593233068303 That looks like an error.\n",
      "Error! Theta element # 2489 , with value  0.0974070591379 has finite difference gradient -0.000123999678297 but analytic gradient is -0.00119492768685 That looks like an error.\n",
      "Error! Theta element # 1470 , with value  0.0359129364934 has finite difference gradient -0.00111212255493 but analytic gradient is -0.0113358382033 That looks like an error.\n",
      "Error! Theta element # 451 , with value  -0.078054207325 has finite difference gradient 0.00120192892649 but analytic gradient is 0.0121740254548 That looks like an error.\n",
      "Error! Theta element # 2092 , with value  0.0220364164402 has finite difference gradient 0.000983122660573 but analytic gradient is 0.00998703645876 That looks like an error.\n",
      "Error! Theta element # 1073 , with value  -0.0753123788182 has finite difference gradient 1.07717545611e-05 but analytic gradient is 0.00010184801932 That looks like an error.\n",
      "Error! Theta element # 54 , with value  -0.0935613104621 has finite difference gradient 0.000903344572557 but analytic gradient is 0.00902242149655 That looks like an error.\n",
      "Error! Theta element # 1695 , with value  -0.0766457151317 has finite difference gradient -0.000531226523991 but analytic gradient is -0.00530528832209 That looks like an error.\n",
      "Error! Theta element # 676 , with value  -0.0938481985663 has finite difference gradient 0.00101494654362 but analytic gradient is 0.010234225451 That looks like an error.\n",
      "Error! Theta element # 2317 , with value  -0.0798152060389 has finite difference gradient -0.000286928111909 but analytic gradient is -0.00282247606725 That looks like an error.\n",
      "Error! Theta element # 1298 , with value  -0.0911371787332 has finite difference gradient 0.000189095802918 but analytic gradient is 0.00183564571155 That looks like an error.\n",
      "Error! Theta element # 279 , with value  -0.00195371162469 has finite difference gradient 0.000989385587865 but analytic gradient is 0.00993589349438 That looks like an error.\n",
      "Error! Theta element # 1920 , with value  -0.0892272346783 has finite difference gradient 0.000419521816554 but analytic gradient is 0.00427337429702 That looks like an error.\n",
      "Error! Theta element # 901 , with value  0.0016914427759 has finite difference gradient 0.00114257365894 but analytic gradient is 0.011470825492 That looks like an error.\n",
      "Error! Theta element # 2542 , with value  -0.0854502733261 has finite difference gradient -5.66930907301e-05 but analytic gradient is -0.000570655936876 That looks like an error.\n",
      "Error! Theta element # 1523 , with value  0.0128811001568 has finite difference gradient -0.00121019388365 but analytic gradient is -0.0121597007419 That looks like an error.\n",
      "Error! Theta element # 504 , with value  0.0890391798361 has finite difference gradient 0.00127776730468 but analytic gradient is 0.0128034132251 That looks like an error.\n",
      "Error! Theta element # 2145 , with value  0.0155820537616 has finite difference gradient -0.000634379842967 but analytic gradient is -0.00634290745805 That looks like an error.\n",
      "Error! Theta element # 1126 , with value  0.0980594571231 has finite difference gradient -0.000688094339848 but analytic gradient is -0.00699814825701 That looks like an error.\n",
      "Error! Theta element # 107 , with value  0.0742080400789 has finite difference gradient -0.00158496113201 but analytic gradient is -0.0158408823822 That looks like an error.\n",
      "Error! Theta element # 1748 , with value  0.0991282886192 has finite difference gradient -0.000660821831491 but analytic gradient is -0.00672242758796 That looks like an error.\n",
      "Error! Theta element # 729 , with value  0.0642191481648 has finite difference gradient 0.000914792757844 but analytic gradient is 0.00924674986959 That looks like an error.\n",
      "Error! Theta element # 2370 , with value  0.0997276577156 has finite difference gradient -0.000567112705147 but analytic gradient is -0.00567713202729 That looks like an error.\n",
      "Error! Theta element # 1351 , with value  0.0678565872788 has finite difference gradient -0.000868063344794 but analytic gradient is -0.00883765241548 That looks like an error.\n",
      "Error! Theta element # 332 , with value  -0.0370333072566 has finite difference gradient -0.00110911005522 but analytic gradient is -0.0110222889247 That looks like an error.\n",
      "Error! Theta element # 1973 , with value  0.0659534500638 has finite difference gradient -0.00168904289314 but analytic gradient is -0.0168216432911 That looks like an error.\n",
      "Error! Theta element # 954 , with value  -0.047575520297 has finite difference gradient -0.000232519563689 but analytic gradient is -0.00231068227709 That looks like an error.\n",
      "Error! Theta element # 1576 , with value  -0.054215379234 has finite difference gradient 0.00167282733241 but analytic gradient is 0.0166968855627 That looks like an error.\n",
      "Error! Theta element # 557 , with value  -0.103116485651 has finite difference gradient 0.000664228551135 but analytic gradient is 0.00673859053714 That looks like an error.\n",
      "Error! Theta element # 2198 , with value  -0.0525429826023 has finite difference gradient 0.000635396510282 but analytic gradient is 0.00643676925608 That looks like an error.\n",
      "Error! Theta element # 1179 , with value  -0.10369785735 has finite difference gradient 0.0010323729737 but analytic gradient is 0.010275369421 That looks like an error.\n",
      "Error! Theta element # 160 , with value  -0.0340092029407 has finite difference gradient -0.00015015240145 but analytic gradient is -0.00149668527965 That looks like an error.\n",
      "Error! Theta element # 1801 , with value  -0.102792168343 has finite difference gradient 0.000601491598407 but analytic gradient is 0.00606233343795 That looks like an error.\n",
      "Error! Theta element # 782 , with value  -0.0306037489774 has finite difference gradient -0.000182148467891 but analytic gradient is -0.00175403404514 That looks like an error.\n",
      "Error! Theta element # 2423 , with value  -0.107541109825 has finite difference gradient 0.0024864234694 but analytic gradient is 0.0248955668616 That looks like an error.\n",
      "Error! Theta element # 1404 , with value  -0.0271859398299 has finite difference gradient 0.000679213757307 but analytic gradient is 0.00658908700243 That looks like an error.\n",
      "Error! Theta element # 385 , with value  0.0758533573913 has finite difference gradient -0.000421075101603 but analytic gradient is -0.00420392866477 That looks like an error.\n",
      "Error! Theta element # 2026 , with value  -0.0304096749105 has finite difference gradient 0.00163443011977 but analytic gradient is 0.0165624157682 That looks like an error.\n",
      "Error! Theta element # 1007 , with value  0.0775826507574 has finite difference gradient -0.000270170595437 but analytic gradient is -0.00268430627307 That looks like an error.\n",
      "Error! Theta element # 1629 , with value  0.0805707075055 has finite difference gradient 9.48056260375e-05 but analytic gradient is 0.00094082686422 That looks like an error.\n",
      "Error! Theta element # 610 , with value  0.0927528320666 has finite difference gradient -0.00142924987966 but analytic gradient is -0.0142372811557 That looks like an error.\n",
      "Error! Theta element # 2251 , with value  0.07867176622 has finite difference gradient 0.00066940906991 but analytic gradient is 0.0067971971 That looks like an error.\n",
      "Error! Theta element # 1232 , with value  0.0878541412159 has finite difference gradient -0.000131900610457 but analytic gradient is -0.00131689364399 That looks like an error.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error! Theta element # 213 , with value  -0.00702761490186 has finite difference gradient 5.30592054453e-05 but analytic gradient is 0.000458014848833 That looks like an error.\n",
      "Error! Theta element # 1854 , with value  0.0846859733448 has finite difference gradient 0.000120111550487 but analytic gradient is 0.0013921657465 That looks like an error.\n",
      "Error! Theta element # 835 , with value  -0.00729960631976 has finite difference gradient -0.000397016810416 but analytic gradient is -0.00392889453741 That looks like an error.\n",
      "Error! Theta element # 2476 , with value  0.0843334105693 has finite difference gradient -0.000492846966954 but analytic gradient is -0.00485038478603 That looks like an error.\n",
      "Error! Theta element # 1457 , with value  -0.00919088711886 has finite difference gradient -0.000573185464564 but analytic gradient is -0.00581785162071 That looks like an error.\n",
      "Error! Theta element # 438 , with value  -0.0868699199718 has finite difference gradient -0.00218727570851 but analytic gradient is -0.021840336507 That looks like an error.\n",
      "Error! Theta element # 2079 , with value  -0.0173124656475 has finite difference gradient 0.00035067121896 but analytic gradient is 0.00369632126635 That looks like an error.\n",
      "Error! Theta element # 1060 , with value  -0.0949256575931 has finite difference gradient -9.25414981739e-05 but analytic gradient is -0.00100296873788 That looks like an error.\n",
      "Error! Theta element # 41 , with value  -0.074326943476 has finite difference gradient 0.00237620519507 but analytic gradient is 0.0236821870446 That looks like an error.\n",
      "Error! Theta element # 1682 , with value  -0.0972519011934 has finite difference gradient 5.76046711874e-05 but analytic gradient is 0.000507437365915 That looks like an error.\n",
      "Error! Theta element # 663 , with value  -0.0656391985194 has finite difference gradient 0.000832998182092 but analytic gradient is 0.00838741596944 That looks like an error.\n",
      "Error! Theta element # 2304 , with value  -0.0973329031944 has finite difference gradient -0.000125070231097 but analytic gradient is -0.00125634469313 That looks like an error.\n",
      "Error! Theta element # 1285 , with value  -0.0622920378072 has finite difference gradient -2.0041834442e-05 but analytic gradient is -0.000254606547767 That looks like an error.\n",
      "Error! Theta element # 266 , with value  0.0410684638653 has finite difference gradient 0.000616776933429 but analytic gradient is 0.00618818029067 That looks like an error.\n",
      "Error! Theta element # 1907 , with value  -0.0557567152252 has finite difference gradient -0.000673101748676 but analytic gradient is -0.00660402166511 That looks like an error.\n",
      "Error! Theta element # 888 , with value  0.0374496317811 has finite difference gradient 0.00283280890856 but analytic gradient is 0.0283488439998 That looks like an error.\n",
      "Error! Theta element # 2529 , with value  -0.0558857286107 has finite difference gradient 0.000515050341017 but analytic gradient is 0.00518127438503 That looks like an error.\n",
      "Error! Theta element # 1510 , with value  0.0639344477914 has finite difference gradient -0.00332989081525 but analytic gradient is -0.0337468158355 That looks like an error.\n",
      "Error! Theta element # 491 , with value  0.0918966931834 has finite difference gradient 0.0015393704552 but analytic gradient is 0.0154372935584 That looks like an error.\n",
      "Error! Theta element # 2132 , with value  0.0536949289221 has finite difference gradient -0.000192065150447 but analytic gradient is -0.00179873897199 That looks like an error.\n",
      "Error! Theta element # 1113 , with value  0.0988859747406 has finite difference gradient 0.000133821160589 but analytic gradient is 0.00119793946486 That looks like an error.\n",
      "Error! Theta element # 94 , with value  0.0276559311697 has finite difference gradient 0.00140935232627 but analytic gradient is 0.0140104103188 That looks like an error.\n",
      "Error! Theta element # 1735 , with value  0.100252438529 has finite difference gradient -0.000123912192559 but analytic gradient is -0.00123670933039 That looks like an error.\n",
      "Error! Theta element # 716 , with value  0.0269790479168 has finite difference gradient 0.000785755380307 but analytic gradient is 0.00795295946329 That looks like an error.\n",
      "Error! Theta element # 2357 , with value  0.100763341499 has finite difference gradient -0.000907062920931 but analytic gradient is -0.00902361514741 That looks like an error.\n",
      "Error! Theta element # 1338 , with value  0.0262709807348 has finite difference gradient -0.000615561313396 but analytic gradient is -0.00629562962642 That looks like an error.\n",
      "Error! Theta element # 319 , with value  -0.0805932956093 has finite difference gradient 0.000714779851469 but analytic gradient is 0.0072737546696 That looks like an error.\n",
      "Error! Theta element # 1960 , with value  0.0298496460885 has finite difference gradient -0.00208570631913 but analytic gradient is -0.020819302176 That looks like an error.\n",
      "Error! Theta element # 941 , with value  -0.075055061553 has finite difference gradient -0.000961980930953 but analytic gradient is -0.00947809196713 That looks like an error.\n",
      "Error! Theta element # 1563 , with value  -0.086487347048 has finite difference gradient 0.00209728755181 but analytic gradient is 0.0209031843913 That looks like an error.\n",
      "Error! Theta element # 544 , with value  -0.0873318094164 has finite difference gradient -9.98461825637e-05 but analytic gradient is -0.000955068802949 That looks like an error.\n",
      "Error! Theta element # 2185 , with value  -0.0836917373781 has finite difference gradient 6.00819213034e-05 but analytic gradient is 0.000687484869107 That looks like an error.\n",
      "Error! Theta element # 1166 , with value  -0.0791468793187 has finite difference gradient -0.00127443247765 but analytic gradient is -0.0129397431957 That looks like an error.\n",
      "Error! Theta element # 147 , with value  0.00990396495687 has finite difference gradient -0.000263005031069 but analytic gradient is -0.00265708770548 That looks like an error.\n",
      "Error! Theta element # 1788 , with value  -0.0844683330218 has finite difference gradient -7.07459428463e-05 but analytic gradient is -0.000697354045719 That looks like an error.\n",
      "Error! Theta element # 769 , with value  0.0125280092222 has finite difference gradient -0.000148881946056 but analytic gradient is -0.00148944173161 That looks like an error.\n",
      "Error! Theta element # 2410 , with value  -0.0912994825324 has finite difference gradient 0.00266218976092 but analytic gradient is 0.0266598457441 That looks like an error.\n",
      "Error! Theta element # 1391 , with value  0.0155940960885 has finite difference gradient 0.000395260796408 but analytic gradient is 0.00388009799673 That looks like an error.\n",
      "Error! Theta element # 372 , with value  0.0965973344324 has finite difference gradient -0.000116061528268 but analytic gradient is -0.00107182420728 That looks like an error.\n",
      "Error! Theta element # 2013 , with value  0.0134366565249 has finite difference gradient 0.000807508722812 but analytic gradient is 0.00819574771171 That looks like an error.\n",
      "Error! Theta element # 994 , with value  0.0932149281506 has finite difference gradient 0.00104378419448 but analytic gradient is 0.0105148454685 That looks like an error.\n",
      "Error! Theta element # 1616 , with value  0.0959824010044 has finite difference gradient 0.000560557312075 but analytic gradient is 0.00555995145419 That looks like an error.\n",
      "Error! Theta element # 597 , with value  0.0595121872656 has finite difference gradient 0.00049669968605 but analytic gradient is 0.00503875431131 That looks like an error.\n",
      "Error! Theta element # 2238 , with value  0.0922002334022 has finite difference gradient 0.0011202253095 but analytic gradient is 0.0113370538191 That looks like an error.\n",
      "Error! Theta element # 1219 , with value  0.0636756422353 has finite difference gradient -0.00104869062972 but analytic gradient is -0.0107131036553 That looks like an error.\n",
      "Error! Theta element # 200 , with value  -0.0462311042121 has finite difference gradient 1.33793610853e-05 but analytic gradient is 0.000148945676818 That looks like an error.\n",
      "Error! Theta element # 1841 , with value  0.056149384824 has finite difference gradient 7.11583196793e-06 but analytic gradient is 7.0297070168e-05 That looks like an error.\n",
      "Gradient test passed. That means that the gradient that your code computed is within 0.001% of the gradient that the finite difference approximation computed, so the gradient calculation procedure is probably correct (not certainly, but probably).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1dc413909b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on the  training data is  2.29439873649\n",
      "The classification error rate on the training data is  0.878\n",
      "The loss on the  validation data is  2.29500096722\n",
      "The classification error rate on the validation data is  0.88\n",
      "The loss on the  test data is  2.29463898534\n",
      "The classification error rate on the test data is  0.8763333333333333\n"
     ]
    }
   ],
   "source": [
    "a3(0, 10, 70, 0.005, 0, False, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on the  training data is  2.30258509299\n",
      "The classification error rate on the training data is  0.9\n",
      "The loss on the  validation data is  2.30258509299\n",
      "The classification error rate on the validation data is  0.9\n",
      "The loss on the  test data is  2.30258509299\n",
      "The classification error rate on the test data is  0.9\n"
     ]
    }
   ],
   "source": [
    "a3(0, 0, 0, 0, 0, False, 0) #Q2 2.30258509299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After  7 optimization iterations, training data loss is 2.30375939536 , and validation data loss is  2.30386659674\n",
      "After  14 optimization iterations, training data loss is 2.30215550774 , and validation data loss is  2.30234317493\n",
      "After  21 optimization iterations, training data loss is 2.30094825708 , and validation data loss is  2.30122138346\n",
      "After  28 optimization iterations, training data loss is 2.29995563575 , and validation data loss is  2.30027906771\n",
      "After  35 optimization iterations, training data loss is 2.29908065068 , and validation data loss is  2.29941342445\n",
      "After  42 optimization iterations, training data loss is 2.29791892122 , and validation data loss is  2.29832249339\n",
      "After  49 optimization iterations, training data loss is 2.29679760351 , and validation data loss is  2.29729039242\n",
      "After  56 optimization iterations, training data loss is 2.29596382508 , and validation data loss is  2.29648480303\n",
      "After  63 optimization iterations, training data loss is 2.29516078074 , and validation data loss is  2.29576295038\n",
      "After  70 optimization iterations, training data loss is 2.29439873649 , and validation data loss is  2.29500096722\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b65be7cb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on the  training data is  2.29439873649\n",
      "The classification error rate on the training data is  0.878\n",
      "The loss on the  validation data is  2.29500096722\n",
      "The classification error rate on the validation data is  0.88\n",
      "The loss on the  test data is  2.29463898534\n",
      "The classification error rate on the test data is  0.8763333333333333\n"
     ]
    }
   ],
   "source": [
    "a3(0, 10, 70, 0.005, 0, False, 4) #Q3 2.29439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After  100 optimization iterations, training data loss is 0.899560217055 , and validation data loss is  1.07058477942\n",
      "After  200 optimization iterations, training data loss is 0.66412269314 , and validation data loss is  0.917596425051\n",
      "After  300 optimization iterations, training data loss is 0.333487647097 , and validation data loss is  0.670347309835\n",
      "After  400 optimization iterations, training data loss is 0.186519367998 , and validation data loss is  0.558950249533\n",
      "After  500 optimization iterations, training data loss is 0.128000358052 , and validation data loss is  0.549008559579\n",
      "After  600 optimization iterations, training data loss is 0.0929182205796 , and validation data loss is  0.552258146368\n",
      "After  700 optimization iterations, training data loss is 0.0756197727686 , and validation data loss is  0.562103914364\n",
      "After  800 optimization iterations, training data loss is 0.0621708722977 , and validation data loss is  0.570890531135\n",
      "After  900 optimization iterations, training data loss is 0.0548661248294 , and validation data loss is  0.581128486796\n",
      "After  1000 optimization iterations, training data loss is 0.0513035047951 , and validation data loss is  0.592698753603\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8EVEYkKBLyISEwp8EZGYUOCLiMTE/wOeeWiWbMHLGAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b65c91c0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on the  training data is  0.0513035047951\n",
      "The classification error rate on the training data is  0.013\n",
      "The loss on the  validation data is  0.592698753603\n",
      "The classification error rate on the validation data is  0.107\n",
      "The loss on the  test data is  0.632910204563\n",
      "The classification error rate on the test data is  0.11355555555555556\n"
     ]
    }
   ],
   "source": [
    "a3(0, 200, 1000, 0.35, 0.9, False, 100) #Q6 0.592698"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping: validation loss was lowest after  507 iterations. We chose the model that we had then.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8EVEYkKBLyISEwp8EZGYUOCLiMTE/wOeeWiWbMHLGAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b65c686048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on the  training data is  0.124530410097\n",
      "The classification error rate on the training data is  0.03\n",
      "The loss on the  validation data is  0.54726000317\n",
      "The classification error rate on the validation data is  0.12\n",
      "The loss on the  test data is  0.582026164368\n",
      "The classification error rate on the test data is  0.12266666666666666\n"
     ]
    }
   ],
   "source": [
    "a3(0, 200, 1000, 0.35, 0.9, True, 100) #Q7 0.547260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
